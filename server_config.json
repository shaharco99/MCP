{
  "defaults": {
    "docker_host": "npipe:////./pipe/docker_engine",
    "model": "gpt-oss",
    "namespace": "jenkins",
    "provider": "ollama",
    "server": "local-server"
  },
  "mcpServers": {
    "filesystem": {
      "args": [
        "-y",
        "@modelcontextprotocol/server-filesystem",
        "C:/Users/shahar/Repos"
      ],
      "command": "npx",
      "description": "Filesystem MCP server for read/write access to local repository",
      "type": "stdio"
    },
    "git": {
      "args": [
        "-y",
        "git-mcp-server@latest"
      ],
      "command": "npx",
      "type": "stdio"
    },
    "kubernetes": {
      "args": [
        "-y",
        "kubernetes-mcp-server@latest"
      ],
      "command": "npx",
      "type": "stdio"
    },
    "local-server": {
      "description": "Local DevOps MCP server with tools",
      "singleResponsePerToolCall": true,
      "toolTimeout": 120,
      "type": "stdio",
      "url": "http://localhost:8000/mcp"
    },
    "playwright": {
      "args": [
        "@playwright/mcp@latest"
      ],
      "command": "npx",
      "type": "stdio",
      "version": "0.0.1-seed"
    },
    "terraform": {
      "args": [
        "-y",
        "terraform-mcp-server@latest"
      ],
      "command": "npx",
      "type": "stdio"
    }
  },
  "providers": {
    "anthropic": {
      "baseUrl": "https://api.anthropic.com",
      "description": "Anthropic Claude provider",
      "models": {
        "claude-instant-v1": {
          "maxTokens": 1500,
          "temperature": 0.5
        },
        "claude-v1": {
          "maxTokens": 3000,
          "temperature": 0.7
        }
      }
    },
    "gemini": {
      "baseUrl": "https://gemini.google.com/api",
      "description": "Google Gemini LLM",
      "models": {
        "gemini-2.5-flash": {
          "maxTokens": 1500,
          "temperature": 0.5
        },
        "gemini-2.5-pro": {
          "maxTokens": 3000,
          "temperature": 0.7
        }
      }
    },
    "ollama": {
      "baseUrl": "http://localhost:11434",
      "description": "Local Ollama LLM provider for chat and prompts",
      "models": {
        "gpt-oss": {
          "maxTokens": 1024,
          "temperature": 0.7,
          "version": "2.0"
        }
      }
    },
    "openai": {
      "baseUrl": "https://api.openai.com/v1",
      "description": "OpenAI API provider",
      "models": {
        "gpt-3.5-turbo": {
          "maxTokens": 2000,
          "temperature": 0.5
        },
        "gpt-4": {
          "maxTokens": 2000,
          "temperature": 0.7
        },
        "gpt-5": {
          "maxTokens": 4000,
          "temperature": 0.7
        }
      }
    }
  }
}
